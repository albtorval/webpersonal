---
title: Análisis de Componentes Principales - resumen teórico
author: Alberto Torrejón Valenzuela
date: '2020-11-13'
slug: análisis-de-componentes-principales-resumen-teórico
categories:
  - Estadística
tags:
  - Minería Estadística
  - R
  - ACP
description: 'Resumen teórico sobre el análisis de componentes principales'
mathjax: local
output:
  blogdown::html_page:
      toc: TRUE
---


<div id="TOC">
<ul>
<li><a href="#análisis-de-componentes-principales---acp">Análisis de Componentes Principales - ACP</a><ul>
<li><a href="#formalización">Formalización</a></li>
<li><a href="#cálculo-de-las-componentes-principales">Cálculo de las Componentes Principales</a></li>
<li><a href="#componentes-principales-en-la-práctica">Componentes principales en la práctica</a></li>
<li><a href="#selección-del-número-de-cp">Selección del número de CP</a></li>
</ul></li>
<li><a href="#referencias">Referencias</a></li>
</ul>
</div>

<div id="análisis-de-componentes-principales---acp" class="section level1">
<h1>Análisis de Componentes Principales - ACP</h1>
<p>En está técnica de aprendizaje no supervisado/reducción de la dimensionalidad se parte de <span class="math inline">\(p\)</span> variables <strong>correlacionadas</strong> (que miden información común) se plantea como objetivo transformar el conjunto original en otro conjunto de <span class="math inline">\(m&lt;p\)</span> nuevas variables incorreladas entre sí (que no tenga repetición o redundancia en la información) llamado conjunto de <strong>componentes principales</strong>, que son combinaciones lineales de las <span class="math inline">\(p\)</span> originales.</p>
<p>La hipótesis de correlación es fundamental. Si las variables originales están incorreladas, entonces carece de sentido calcular unos componentes principales. Si se hiciera, se obtendrían las mismas variables pero reordenadas de mayor a menor varianza. La hipótesis de normalidad de los datos no es necesaria, aunque si se cumple la interpretación de los resultados proporciona mayor información.</p>
<div id="formalización" class="section level2">
<h2>Formalización</h2>
<p>Dado un vector aleatorio <span class="math inline">\(p\)</span>-dimensional de variables aleatorias <strong>continuas</strong> <span class="math display">\[\underline{X} = (X_1, ..., X_p)&#39;.\]</span></p>
<p>Queremos transformarlo en otro vector <span class="math inline">\(\underline{Y} = (Y_1, ..., Y_p)&#39;\)</span> donde las <span class="math inline">\(Y_i\)</span> son combinaciones lineales de <span class="math inline">\(X_1,...,X_p\)</span> en orden descendente de importancia. Luego seleccionaremos las <span class="math inline">\(m&lt;p\)</span> componentes que superen un determinado umbral de importancia.</p>
<p>Se supone que:</p>
<ul>
<li><span class="math inline">\(\exists \ \underline{\mu} = E[\underline{X}] = (\mu_1,...,\mu_p)&#39;\)</span> <strong>vector de medias</strong>, donde <span class="math inline">\(\mu_i = E[X_i].\)</span></li>
<li><span class="math inline">\(\exists \ \mathbf{\Sigma}\)</span> <strong>matriz de covarianzas</strong> de <span class="math inline">\(\underline{X}\)</span> dada por:</li>
</ul>
<p><span class="math display">\[\mathbf{\Sigma} = Cov(\underline{X}) = \begin{pmatrix}
\sigma_{11} &amp; ... &amp; \sigma_{1p} \\ 
\vdots &amp; \ddots &amp; \vdots \\ 
\sigma_{p1} &amp; ... &amp; \sigma_{pp}
\end{pmatrix}.\]</span> con</p>
<ul>
<li><span class="math inline">\(\sigma_{ij} = Cov(X_i, X_j) = E[(X_i-\mu_i)(X_j-\mu_j)].\)</span></li>
<li><span class="math inline">\(\sigma_{ii} = Cov(X_i,X_i) = Var(X_i).\)</span></li>
</ul>
<p><strong>Notación:</strong> <span class="math inline">\(\underline{X} \sim (\underline\mu, \mathbf{\Sigma}).\)</span></p>
<p>Por otra parte, se define la <strong>matriz de correlaciones</strong> como</p>
<p><span class="math display">\[\mathbf{R} = \begin{pmatrix}
\rho_{11} &amp; ... &amp; \rho_{1p} \\ 
\vdots &amp; \ddots  &amp; \vdots \\ 
\rho_{p1} &amp; ... &amp; \rho_{pp}
\end{pmatrix}\]</span></p>
<p>donde</p>
<ul>
<li><span class="math inline">\(\rho_{jj} = 1 \quad \forall i=1,...,p\)</span>.</li>
<li><span class="math inline">\(\rho_{jk} = \frac{\sigma_{jk}}{\sigma_j \sigma_k} \ \)</span> si <span class="math inline">\(\ j\neq k\)</span>.</li>
</ul>
<p>Si <span class="math inline">\(\rho_{ij}^2 = 0\)</span> significa que las variables <span class="math inline">\(X_i\)</span> y <span class="math inline">\(X_j\)</span> están incorreladas. Por el contrario, si <span class="math inline">\(\rho_{ij}^2 = 1\)</span> entonces existe una dependencia lineal entre ellas <span class="math inline">\((X_i = a + bX_j \quad a,b\in \mathbb{R})\)</span>.</p>
<p>En forma matricial se tiene que <span class="math inline">\(\mathbf{R} = \Delta^{-1}\mathbf{\Sigma}\Delta^{-1}\)</span> con <span class="math inline">\(\Delta = diag\{\sigma_1,\cdots,\sigma_p\}\)</span>.</p>
<p>Otro concepto importante en la formalización será la <strong>estandarización</strong> de las variables, adimensionar las variables para facilitar su estudio. Se consigue considerando la variable centrada por su media y escalada por su varianza:</p>
<p><span class="math display">\[\underline{X}_{(s)} = \begin{bmatrix}
\frac{X_1-\mu_1}{\sigma_1}\\ 
\vdots \\ 
\frac{X_p-\mu_p}{\sigma_p}
\end{bmatrix} = \Delta^{-1}(\underline{X}-\underline\mu).\]</span></p>
<p>Se cumple la siguiente relación <span class="math inline">\(\mathbf{R} = \mathbf{\Sigma}_{(s)} = \mathbf{R}_{(s)}\)</span></p>

<div class="definition">
<p><span id="def:unnamed-chunk-1" class="definition"><strong>Definición 1  </strong></span>La <span class="math inline">\(j\)</span>-ésima <strong>Componente Principal</strong> (PC), <span class="math inline">\(Y_j\)</span>, se define como la combinación lineal:</p>
<p><span class="math display">\[
\begin{align}
  Y_j &amp; =  t_{j1}X_1 + ... + t_{jp}X_p \\
      &amp; = \underline{t}_j&#39; \underline{X}
\end{align} \\
\]</span></p>
donde <span class="math inline">\(\underline{t}_j&#39; = (t_{j1},..., t_{jp})&#39; \quad t_{ij} \in \mathbb{R}\)</span>
</div>

<p>A continuación se presentan algunas propiedades de estas tranformaciones lineales.</p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-2" class="proposition"><strong>Proposición 1  </strong></span>Sea <span class="math inline">\(\underline{X} \sim (\underline\mu, \bf\Sigma)\)</span>,</p>
<span class="math display">\[\begin{align}
E[Y_j] &amp; = E[\underline{t}_j&#39;\underline{X}] = \underline{t}_j&#39;\underline\mu \\
Var(Y_j) &amp; = Var(\underline{t}_j&#39;\underline{X}) = \underline{t}_j&#39; \mathbf{\Sigma} \underline{t}_j \\
Cov(Y_i,Y_j) &amp; = Cov(\underline{t}_i&#39;\underline{X},\underline{t}_j&#39;\underline{X}) = \underline{t}_i&#39; \mathbf{\Sigma} \underline{t}_j
\end{align}\]</span>
</div>

<p>Para demostrar esta proposición es necesario recurrir a las formas matriciales cuadráticas. <span class="math inline">\(\mathbf{\Sigma} \in \mathcal{M}^{p\times p}\)</span> <strong>simétrica</strong> y <strong>semidefinida positiva</strong>, por lo que por el teorema espectral podemos dar una representación de la matriz en términos de sus autovalores que son reales y positivos,</p>
<p><span class="math display">\[\lambda_1 \geq \lambda_2 \geq  \cdots \geq \lambda_p \geq 0.\]</span></p>
<p>La suma total de todos los autovalores es <span class="math inline">\(p\)</span>. Los autovectores que pertenecen a los autovalores más grandes indican los principal dirección de los datos. Sea <span class="math inline">\(\underline{e}_i\)</span> el autovector unitario, <span class="math inline">\(||\underline{e}_i||^2 = 1\)</span>, asociado a <span class="math inline">\(\lambda_i\)</span>. Los autovectores son <strong>ortogonales</strong>, <span class="math inline">\(\underline{e}_i&#39;\underline{e}_j = 0\)</span>, si <span class="math inline">\(i\neq j\)</span>. Sea <span class="math inline">\(E_{p\times p} = [\underline{e}_1,\underline{e}_2,\cdots,\underline{e}_p]\)</span>, <span class="math inline">\(E\)</span> es ortogonal ya que <span class="math inline">\(\mathbf{E}&#39;\mathbf{E} = \mathbf{E}\mathbf{E}&#39; = I.\)</span> De hecho tenemos el siguiente resultado</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Teorema 1  </strong></span><strong>(Teorema de la descomposición espectral)</strong></p>
<p><span class="math display">\[\mathbf\Sigma =  \mathbf{E} \mathbf\Lambda \mathbf{E}&#39; = \sum_{i=1}^p \lambda_i \underline{e}_i \underline{e}_i&#39;\]</span></p>
<p>donde <span class="math inline">\(\mathbf\Lambda = diag\{\lambda_1,\cdots,\lambda_p\}\)</span>, con <span class="math inline">\(\lambda_i\)</span> autovalores.</p>
<p>Además:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbf{E}&#39; \mathbf\Sigma \mathbf{E} = \mathbf\Lambda\)</span></li>
<li><span class="math inline">\(tr(\mathbf\Sigma) = tr(\mathbf\Lambda) = \sum_{j=1}^p \lambda_j\)</span></li>
</ol>
</div>

</div>
<div id="cálculo-de-las-componentes-principales" class="section level2">
<h2>Cálculo de las Componentes Principales</h2>
<p>La primera componente principal <span class="math inline">\(Y_1 = t_{11}X_1 + ... + t_{1p}X_p\)</span> con <span class="math inline">\(t_{1i} \in \mathbb{R}\)</span> es aquella cuya varianza <span class="math inline">\(var[Y_1] = \underline{t}_1&#39; \mathbf{\Sigma} \underline{t}_1\)</span> es máxima. Se impone la condición de que los autovectores sean unitarios para acotar los valores, i.e. <span class="math inline">\(\underline{t}_1&#39;\underline{t}_1 = 1\)</span>. Luego se ha de resolver el siguiente problema:</p>
<p>Obtener <span class="math inline">\(\underline{t}_1 = (t_{11}, \cdots , t_{1p})\)</span> tal que:</p>
<p><span class="math display">\[\begin{align} &amp; max(\underline{t}_1&#39; \mathbf{\Sigma} \underline{t}_1) \\ &amp; s.a.: \\ &amp; \underline{t}_1&#39;\underline{t}_1 = 1 \end{align}\]</span></p>
<p>Aplicando el <em>Método de los Multiplicadores de Lagrange</em>, véase la resolución <a href="http://halweb.uc3m.es/esp/Personal/personas/jmmarin/esp/AMult/tema3am.pdf">aquí</a>, <span class="math inline">\(\underline{t}_1\)</span> es el vector propio unitario correspondiente al valor propio más grande <span class="math inline">\(\lambda_1\)</span> de la matriz de covarianza <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<p>La segunda componente principal tiene que capturar la mayor parte de la variabilidad no capturada por <span class="math inline">\(Y_1\)</span>, por lo que <span class="math inline">\(Y_2\)</span> debe estar incorrelada con <span class="math inline">\(Y_1\)</span>, i.e. <span class="math inline">\(Cov(Y_2,Y_1) = \underline{t}_2&#39; \mathbf{\Sigma} \underline{t}_1 = 0\)</span>. Luego para obtener <span class="math inline">\(\underline{t}_2 = (t_{21}, \cdots , t_{2p})\)</span> resolvemos:</p>
<p><span class="math display">\[\begin{align} &amp; max(\underline{t}_2&#39; \mathbf{\Sigma} \underline{t}_2) \\ &amp; s.a.: \\ &amp; \underline{t}_2&#39;\underline{t}_2 = 1 \\ &amp; \underline{t}_2&#39; \mathbf{\Sigma} \underline{t}_1 = 0 \end{align}\]</span>
<span class="math inline">\(t_2\)</span> es el vector propio unitario correspondiente al segundo valor propio más grande <span class="math inline">\(\lambda_2\)</span> de la matriz de covarianza <span class="math inline">\(\mathbf{\Sigma}\)</span>. Y de está forma, suponiendo que el resto de componentes están incorreladas con las anteriores se van calculando el resto de componentes. La <span class="math inline">\(j\)</span>-ésima componente principal será el resultado de</p>
<p><span class="math display">\[\begin{align} &amp; max(\underline{t}_j&#39; \mathbf{\Sigma} \underline{t}_j) \\ &amp; s.a.: \\ &amp; \underline{t}_j&#39;\underline{t}_j = 1 \\ &amp; \underline{t}_j&#39; \mathbf\Sigma \underline{t}_h = 0 \quad h = 1,...,j-1 \end{align}\]</span>
Véase el <strong>Teorema de la <span class="math inline">\(j\)</span>-ésima componente principal</strong>.</p>
<p><strong><u>Propiedades</u></strong></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Var[Y_j] = \lambda_j\)</span>.</li>
<li>Las PC están incorreladas.</li>
<li><span class="math inline">\(Var[Y_1] \leq Var[Y_2] \leq \cdots \leq Var[Y_p]\)</span>.</li>
</ol>
</div>
<div id="componentes-principales-en-la-práctica" class="section level2">
<h2>Componentes principales en la práctica</h2>
<p>Dada una realización muestral de tamaño <span class="math inline">\(n\)</span> del vector aleatorio <span class="math inline">\(\underline{X}\)</span> se tiene la siguiente notación:</p>
<p><span class="math display">\[\mathbf{X}_{n\times p} = \begin{pmatrix}
x_{11} &amp; ... &amp; x_{1p} \\ 
\vdots &amp; \ddots  &amp; \vdots \\ 
x_{n1} &amp; ... &amp; x_{np}
\end{pmatrix} = 
\underbrace{\begin{pmatrix}
\underline{x}_1 \\ 
\vdots \\ 
\underline{x}_n
\end{pmatrix}}_\text{muestra individuos} = \underbrace{\begin{pmatrix}
\underline{x}_{(1)}, ... , \underline{x}_{(p)} 
\end{pmatrix}}_\text{muestra variables}\]</span></p>
<p>En la práctica, la técnica de las CP debe aplicarse reemplazando las características de la población por sus respectivos estimadores, es decir <span class="math inline">\(\mu \rightarrow \hat\mu = \overline{\underline{x}}\)</span> y <span class="math inline">\(\mathbf{\Sigma} \rightarrow \mathbf{\hat{\Sigma}}\)</span> con</p>
<ul>
<li><span class="math inline">\(\overline{\underline{x}} = (\overline{x}_1,...,\overline{x}_p)&#39;.\)</span></li>
<li><span class="math inline">\(\mathbf{\hat{\Sigma}} = \frac{1}{n-1}\sum_{i=1}^n(\underline{x}_i - \overline{\underline{x}})(\underline{x}_i - \overline{\underline{x}})&#39;.\)</span></li>
</ul>
<p>Como <span class="math inline">\(\mathbf{\hat\Sigma}\)</span> sigue siendo una marriz <span class="math inline">\(p\times p\)</span> simétrica y semidefinida positiva se aplican los mismos resultados anteriores de forma que se tiene que la <span class="math inline">\(i\)</span>-ésima CP muestral será <span class="math inline">\(Y_i = \underline{\hat e}_i&#39;\underline{X}\)</span> para <span class="math inline">\(i=1,...,p\)</span>. Las <span class="math inline">\(\underline{\hat e}_i\)</span> son conocidas como <strong>cargas</strong> - <em>loadings</em> - que son los coeficientes que definen cada componente principal. Al final del proceso las variables originales <span class="math inline">\(\underline{X}\)</span> se habrán tranformado en otras nuevas <span class="math inline">\(\underline{Y}\)</span>.</p>
<p><span class="math display">\[\underline{X} = \begin{pmatrix} X_1 \\ \vdots \\ X_p \end{pmatrix} \rightarrow \underline{Y} =  \begin{pmatrix} \underline{\hat e}_1&#39;\underline{X} \\ \vdots \\ \underline{\hat e}_p&#39;\underline{X} \end{pmatrix}\]</span>
Las conocidas como <strong>puntuaciones</strong> - <em>scores</em> - se recogen en la matriz</p>
<p><span class="math display">\[ \mathbf{Y} = \begin{pmatrix}
\underline{x}_1&#39;\underline{\hat{e}}_1 &amp; \cdots  &amp; \underline{x}_1&#39;\underline{\hat{e}}_p \\ 
\vdots &amp; \ddots  &amp; \vdots \\ 
\underline{x}_n&#39;\underline{\hat{e}}_1 &amp; \cdots  &amp; \underline{x}_n&#39;\underline{\hat{e}}_p
\end{pmatrix} = \mathbf{X\hat{E}}\]</span></p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-4" class="proposition"><strong>Proposición 2  </strong></span>Dado que <span class="math inline">\(\mathbf{X\hat{E}}\)</span> es una matriz ortogonal, las distancias entre los datos originales y los datos transformados concuerdan</p>
<p><span class="math display">\[d^2(y_i, y_j) = d^2(x_i, x_j)\]</span></p>
<strong>Observación</strong>: Esta propiedad es útil para detectar valores atípicos y grupos mediante CP.
</div>

<p>Las CP no son en general invariantes respecto a cambios de escala. Si multiplicamos una variable por un escalar obtenemos diferentes autovalores y autovectores. Esto se debe al hecho de que se realiza una descomposición de valores propios en la matriz de covarianza y no en la matriz de correlación. Se puede realizar el estudio de las CP a partir de la matriz de datos estandarizados, lo que equivale a trabajar con la matriz de correlación. Entonces, en la práctica, deberíamos considerar la matriz que contiene los datos estandarizados.</p>
<p><span class="math display">\[\begin{pmatrix}
\frac{x_{11}-\overline{x}_1}{\hat\sigma_1} &amp; \cdots &amp; \frac{x_{1p}-\overline{x}_p}{\hat\sigma_p} \\ 
\vdots &amp; \ddots &amp; \vdots \\ 
\frac{x_{n1}-\overline{x}_1}{\hat\sigma_1} &amp; \cdots &amp; \frac{x_{np}-\overline{x}_p}{\hat\sigma_p}
\end{pmatrix}\]</span></p>
<p>Cuando las escalas de medida de las variables son muy distintas, las variables con valores más grandes tendrán más peso en el análisis. Se estandariza para que las magnitudes sean similares. La estandarización resuelve otro posible problema. Si las variabilidades de los datos originales son muy distintas, las variables con mayor varianza van a influir más en la determinación de la primera componente principal, se estandariza para evitarlo.</p>
<p>Para datos estandarizados se tiene que <span class="math inline">\(\sum_{j=1}^p\hat\lambda_j = tr(\mathbf{\hat\Sigma}) = tr(\mathbf{\hat R}) = p\)</span>.</p>
</div>
<div id="selección-del-número-de-cp" class="section level2">
<h2>Selección del número de CP</h2>
<p>La proporción de la variabilidad explicada por la <span class="math inline">\(k\)</span>-ésima componente principal muestral es</p>
<p><span class="math display">\[\frac{\hat\lambda_k}{\sum_{s=1}^p\hat\lambda_s}.\]</span></p>
<p>La proporción de la variablidad explicada por las <span class="math inline">\(k\)</span> primeras componentes principales muestrales es <span class="math inline">\(\frac{\sum_{s=1}^k\hat\lambda_s}{\sum_{s=1}^p\hat\lambda_s}\)</span>. La no explicada por las <span class="math inline">\(k\)</span> primeras sería <span class="math inline">\(\frac{\sum_{s=k+1}^p\hat\lambda_s}{\sum_{s=1}^p\hat\lambda_s}\)</span>.</p>
<p><u><strong>Criterios</strong></u></p>
<p>Por lo general, los criterios se basan en gráficos para los valores propios - <strong>scree plot</strong>. Algunos criterios destacables son los siguientes:</p>
<ol style="list-style-type: decimal">
<li>Elegir las <span class="math inline">\(k\)</span> primeras si <span class="math inline">\(\frac{\hat\lambda_{k+1}}{\sum_{s=1}^p\hat\lambda_s} &lt; \delta_1\)</span> ó <span class="math inline">\(\frac{\sum_{s=k+1}^p\hat\lambda_{s}}{\sum_{s=1}^p\hat\lambda_s} &lt; \delta_2\)</span> con <span class="math inline">\(\delta_1,\delta_2\)</span> elegidos correctamente.</li>
<li>Retener las componentes suficientes hasta que expliquen un determinado porcentaje de la variabilidad total, 80%.</li>
<li><strong>Regla de Kaiser-Guttman</strong>. Aquellas cuyos autovalores sean mayores que la media <span class="math inline">\(\frac{\sum_{s=1}^p\hat\lambda_s}{p}\)</span>. La media es 1 si las CP se calculan sobre las variables estandarizadas.</li>
</ol>
</div>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<ul>
<li><p>Apuntes de la asignatura de Minería Estadística de Datos del Máster Universitario en Matemáticas de la Universidad de Sevilla por la profesora Inmaculada Barranco Chamorro.</p></li>
<li><p>Apuntes de la asignatura de Análisis de Datos Multivariantes del Grado en Matemáticas de la universidad de Sevilla por los profesores Juan Luis Rebollo Moreno, Juan Manuel Muñoz Pichardo y Rafael Pino Mejías.</p></li>
</ul>
</div>
